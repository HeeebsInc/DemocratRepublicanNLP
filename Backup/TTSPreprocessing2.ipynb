{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline \n",
    "\n",
    "1. apply regex and lower case\n",
    "2. Standard Scaler to volume and DayDiff\n",
    "3. Count Vectorizer to Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 2\n",
    "df = pd.read_csv('FData/SPYHeadGrouped.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIPELINE FUNCTIONS\n",
    "def preprocess_headline(headline): \n",
    "    reg_token = RegexpTokenizer(\"([a-zA-Z&]+(?:'[a-z]+)?)\")\n",
    "\n",
    "    new_headline = ' '.join([i for i in headline.lower().split() if i != 'rt' and i.endswith('â€¦') == False])\n",
    "    new_headline  = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",new_headline.lower()).split())\n",
    "    new_headline  = reg_token.tokenize(new_headline .lower())\n",
    "    \n",
    "    word_stem = PorterStemmer()\n",
    "#     word_lem = WordNetLemmatizer()\n",
    "#     new_tweet= ' '.join([word_lem.lemmatize(i) for i in new_tweet])\n",
    "#     new_headline = [word_stem.stem(i) for i in new_headline if len(i) > 1]\n",
    "\n",
    "#     print(f'{tweet}\\n')\n",
    "#     print(f'{new_tweet}\\n')\n",
    "#     print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "#     return new_tweet\n",
    "    \n",
    "    return ' '.join(new_headline)\n",
    "\n",
    "def new_headline(df): \n",
    "    df['Headlines'] = df.Headlines.map(preprocess_headline)\n",
    "    return df\n",
    "\n",
    "df = new_headline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Headlines', 'Volume', 'DayDiff']]\n",
    "Y = df[['Target']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, stratify = Y.Target.values, random_state = 10, train_size = .70)\n",
    "\n",
    "print(f'Train:\\t{len(x_train)}\\n{y_train.Target.value_counts()}\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print(f'Test:\\t{len(x_test)}\\n{y_test.Target.value_counts()}\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for original change max_features back to 3000\n",
    "\n",
    "def get_preprocessing_pickles(pick_name, x_train, x_test, y_train, y_test, new = True):\n",
    "    if new == True:    \n",
    "        ss_volume = StandardScaler().fit(x_train['Volume'].values.reshape(-1,1))\n",
    "        ss_daydiff = StandardScaler().fit(x_train['DayDiff'].values.reshape(-1,1))\n",
    "        cv = CountVectorizer(stop_words = 'english', max_features = 5000, ngram_range = (1,1), min_df = 2).fit(x_train['Headlines'])\n",
    "        preprocessing_dict = {'ss_volume': ss_volume, 'ss_daydiff': ss_daydiff, 'headlines': cv}\n",
    "        pickle.dump(preprocessing_dict, open(f'Pickles/PreprocessingDict_{pick_name}.p', 'wb'))\n",
    "    else: \n",
    "        preprocessing_dict = pickle.load(open(f'Pickles/PreprocessingDict_{pick_name}.p', 'rb'))\n",
    "    \n",
    "    return preprocessing_dict\n",
    "\n",
    "preprocessing_dict = get_preprocessing_pickles(model_num, x_train, x_test, y_train, y_test, new = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_steps(preprocessing_dict, x_train, x_test, y_train, y_test, pick_name = None): \n",
    "    #standard scaler volume\n",
    "    x_train_new = pd.DataFrame()\n",
    "    x_test_new = pd.DataFrame()\n",
    "    \n",
    "    x_train_new['Volume'] = preprocessing_dict['ss_volume'].transform(x_train.Volume.values.reshape(-1,1)).ravel()\n",
    "    x_test_new['Volume'] = preprocessing_dict['ss_volume'].transform(x_test.Volume.values.reshape(-1,1)).ravel()\n",
    "    \n",
    "    #standard scaler daydiff\n",
    "    x_train_new['DayDiff'] = preprocessing_dict['ss_daydiff'].transform(x_train.DayDiff.values.reshape(-1,1))\n",
    "    x_test_new['DayDiff'] = preprocessing_dict['ss_daydiff'].transform(x_test.DayDiff.values.reshape(-1,1))\n",
    "    \n",
    "    cv_vec = preprocessing_dict['headlines']\n",
    "    train_headlines = pd.DataFrame(cv_vec.transform(x_train['Headlines']).toarray(), columns = cv_vec.get_feature_names())\n",
    "    test_headlines = pd.DataFrame(cv_vec.transform(x_test['Headlines']).toarray(), columns = cv_vec.get_feature_names())\n",
    "\n",
    "    x_train_new = pd.concat([x_train_new, train_headlines], axis = 1)\n",
    "    x_test_new = pd.concat([x_test_new, test_headlines], axis = 1)\n",
    "\n",
    "    if pick_name: \n",
    "        tts = (x_train_new, x_test_new, y_train, y_test)\n",
    "        pickle.dump(tts, open(f'Pickles/TTS_{pick_name}.p', 'wb'))\n",
    "    return x_train_new, x_test_new, y_train, y_test\n",
    "\n",
    "tts = preprocess_steps(preprocessing_dict, x_train, x_test, y_train, y_test, pick_name = model_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_transformer = FunctionTransformer(new_headline)\n",
    "stop = stopwords\n",
    "count_vec = CountVectorizer(stop_words = 'english', max_features = 5000, ngram_range = (1,3))\n",
    "ss_transformer = ColumnTransformer(transformers = [('ss', StandardScaler(), ['Volume', 'DayDiff'])], \n",
    "                                   n_jobs = -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([('feats', FeatureUnion([('cv', cv_transformer), ('ss', ss_transformer)]))])\n",
    "\n",
    "\n",
    "\n",
    "test = pipe.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class TextExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Adapted from code by @zacstewart\n",
    "       https://github.com/zacstewart/kaggle_seeclickfix/blob/master/estimator.py\n",
    "       Also see Zac Stewart's excellent blogpost on pipelines:\n",
    "       http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Select the relevant column and return it as a numpy array.\n",
    "        # Set the array type to be string.\n",
    "        return np.asarray(df[self.column_name]).astype(str)\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class Apply(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in a function and applies it element-wise to every element in the numpy array it's supplied with.\"\"\"\n",
    "\n",
    "    def __init__(self, fn):\n",
    "        self.fn = np.vectorize(fn)\n",
    "\n",
    "    def transform(self, data):\n",
    "        # Note: reshaping is necessary because otherwise sklearn\n",
    "        # interprets the 1-d array as a single sample.\n",
    "        return self.fn(data.reshape(data.size, 1))\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class AverageWordLengthExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts last name column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def average_word_length(self, name):\n",
    "        \"\"\"Helper code to compute average word length of a name\"\"\"\n",
    "        return np.mean([len(word) for word in name.split()])\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        return df['LAST_NAME'].apply(self.average_word_length)\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns self unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "# Let's pick the same random 10% of the data to train with.\n",
    "random.seed(1965)\n",
    "train_test_set = df.loc[random.sample(list(df.index.values), int(len(df) / 10))]\n",
    "\n",
    "# X = train_test_set[['road_name', 'has_malay_road_tag']]\n",
    "X = train_test_set[['LAST_NAME']]\n",
    "y = train_test_set['RACE']\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,4), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('name_extractor', TextExtractor('LAST_NAME')),    # Extract names from df.\n",
    "    ('text_features', FeatureUnion([\n",
    "        ('vect', vect),    # Extract ngrams from names.\n",
    "        ('num_words', Apply(lambda s: len(s.split()))),    # Number of words.\n",
    "        ('ave_word_length', Apply(lambda s: np.mean([len(w) for w in s.split()]))), # Average word length.\n",
    "    ])),\n",
    "    ('clf' , clf),     # Feed the output through a classifier.\n",
    "])\n",
    "\n",
    "def run_experiment(X, y, pipeline, num_expts=100):\n",
    "    scores = list()\n",
    "    for i in range(num_expts):\n",
    "        X_train, X_test, y_train, y_true = train_test_split(X, y)\n",
    "        model = pipeline.fit(X_train, y_train)  # Train the classifier.\n",
    "        y_test = model.predict(X_test)          # Apply the model to the test data.\n",
    "        score = accuracy_score(y_test, y_true)  # Compare the results to the gold standard.\n",
    "        scores.append(score)\n",
    "\n",
    "    print(sum(scores) / num_expts)\n",
    "\n",
    "# Run x times (num_expts) and get the average accuracy.\n",
    "run_experiment(X, y, pipeline, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DemocratRepublicanNLP",
   "language": "python",
   "name": "democratrepublicannlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
