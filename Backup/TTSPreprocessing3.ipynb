{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline \n",
    "\n",
    "1. apply regex and lower case\n",
    "2. Standard Scaler to volume and DayDiff\n",
    "3. Count Vectorizer to Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Headlines</th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>CloseDiff</th>\n",
       "      <th>CloseDiffNew</th>\n",
       "      <th>Target</th>\n",
       "      <th>DayDiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-19</td>\n",
       "      <td>House prices to fall in London and south-east ...</td>\n",
       "      <td>268.48</td>\n",
       "      <td>268.53</td>\n",
       "      <td>267.09</td>\n",
       "      <td>267.17</td>\n",
       "      <td>82382876.0</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-20</td>\n",
       "      <td>Hedge funds fail to stop 'billion-dollar brain...</td>\n",
       "      <td>268.27</td>\n",
       "      <td>268.33</td>\n",
       "      <td>266.69</td>\n",
       "      <td>267.03</td>\n",
       "      <td>76751500.0</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Time                                          Headlines  1. open  \\\n",
       "0  2017-12-19  House prices to fall in London and south-east ...   268.48   \n",
       "1  2017-12-20  Hedge funds fail to stop 'billion-dollar brain...   268.27   \n",
       "\n",
       "   2. high  3. low  4. close      Volume  CloseDiff  CloseDiffNew  Target  \\\n",
       "0   268.53  267.09    267.17  82382876.0      -1.03         -0.14       0   \n",
       "1   268.33  266.69    267.03  76751500.0      -0.14          0.55       1   \n",
       "\n",
       "   DayDiff  \n",
       "0    -1.31  \n",
       "1    -1.24  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_num = 3\n",
    "df = pd.read_csv('../FData/Headlines/New/SPYHeadlinesGrouped1.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIPELINE FUNCTIONS\n",
    "def preprocess_headline(headline): \n",
    "    reg_token = RegexpTokenizer(\"([a-zA-Z&]+(?:'[a-z]+)?)\")\n",
    "\n",
    "    new_headline = ' '.join([i for i in headline.lower().split() if i != 'rt' and i.endswith('â€¦') == False])\n",
    "    new_headline  = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",new_headline.lower()).split())\n",
    "    new_headline  = reg_token.tokenize(new_headline .lower())\n",
    "    \n",
    "    word_stem = PorterStemmer()\n",
    "#     word_lem = WordNetLemmatizer()\n",
    "#     new_tweet= ' '.join([word_lem.lemmatize(i) for i in new_tweet])\n",
    "    new_headline = [word_stem.stem(i) for i in new_headline if len(i) > 1]\n",
    "\n",
    "#     print(f'{tweet}\\n')\n",
    "#     print(f'{new_tweet}\\n')\n",
    "#     print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "#     return new_tweet\n",
    "    \n",
    "    return ' '.join(new_headline)\n",
    "\n",
    "def new_headline(df): \n",
    "    df['Headlines'] = df.Headlines.map(preprocess_headline)\n",
    "    return df\n",
    "\n",
    "df = new_headline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t451\n",
      "1    254\n",
      "0    197\n",
      "Name: Target, dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Test:\t194\n",
      "1    109\n",
      "0     85\n",
      "Name: Target, dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "X = df[['Headlines', 'Volume', 'DayDiff']]\n",
    "Y = df[['Target']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, stratify = Y.Target.values, random_state = 10, train_size = .70)\n",
    "\n",
    "print(f'Train:\\t{len(x_train)}\\n{y_train.Target.value_counts()}\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print(f'Test:\\t{len(x_test)}\\n{y_test.Target.value_counts()}\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_steps(preprocessing_dict, x_train, x_test, y_train, y_test, pick_name = None): \n",
    "    #standard scaler volume\n",
    "    x_train_new = pd.DataFrame()\n",
    "    x_test_new = pd.DataFrame()\n",
    "    \n",
    "    x_train_new['Volume'] = preprocessing_dict['ss_volume'].transform(x_train.Volume.values.reshape(-1,1)).ravel()\n",
    "    x_test_new['Volume'] = preprocessing_dict['ss_volume'].transform(x_test.Volume.values.reshape(-1,1)).ravel()\n",
    "    \n",
    "    #standard scaler daydiff\n",
    "    x_train_new['DayDiff'] = preprocessing_dict['ss_daydiff'].transform(x_train.DayDiff.values.reshape(-1,1))\n",
    "    x_test_new['DayDiff'] = preprocessing_dict['ss_daydiff'].transform(x_test.DayDiff.values.reshape(-1,1))\n",
    "    \n",
    "    cv_vec = preprocessing_dict['headlines']\n",
    "    train_headlines = pd.DataFrame(cv_vec.transform(x_train['Headlines']).toarray(), columns = cv_vec.get_feature_names())\n",
    "    test_headlines = pd.DataFrame(cv_vec.transform(x_test['Headlines']).toarray(), columns = cv_vec.get_feature_names())\n",
    "\n",
    "    x_train_new = pd.concat([x_train_new, train_headlines], axis = 1)\n",
    "    x_test_new = pd.concat([x_test_new, test_headlines], axis = 1)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = 3, max_iter = 1000, tol = 1e-3).fit(x_train_new.values)\n",
    "    preprocessing_dict['k_cluster'] = kmeans\n",
    "    x_test_new['KCluster'] = kmeans.predict (x_test_new.values)\n",
    "    x_train_new['KCluster'] = kmeans.predict(x_train_new.values)\n",
    "\n",
    "    if pick_name: \n",
    "        tts = (x_train_new, x_test_new, y_train, y_test, preprocessing_dict)\n",
    "        pickle.dump(tts, open(f'../Pickles/TTS_{pick_name}.p', 'wb'))\n",
    "#         pickle.dump(preprocessing_dict, open(f'Pickles/PreprocessingDict_{pick_name}.p', 'wb'))\n",
    "\n",
    "    return x_train_new, x_test_new, y_train, y_test, preprocessing_dict\n",
    "\n",
    "\n",
    "def get_preprocessing_pickles(pick_name, x_train, x_test, y_train, y_test):\n",
    "    ss_volume = StandardScaler().fit(x_train['Volume'].values.reshape(-1,1))\n",
    "    ss_daydiff = StandardScaler().fit(x_train['DayDiff'].values.reshape(-1,1))\n",
    "    cv = CountVectorizer(stop_words = 'english', max_features = 5000, ngram_range = (1,1), min_df = 1).fit(x_train['Headlines'])\n",
    "\n",
    "    preprocessing_dict = {'ss_volume': ss_volume, 'ss_daydiff': ss_daydiff, 'headlines': cv}\n",
    "    \n",
    "    x_train_new, x_test_new, y_train, y_test, preprocessing_dict = preprocess_steps(preprocessing_dict, x_train, x_test, \n",
    "                                                                                    y_train, y_test, pick_name = pick_name)\n",
    "    \n",
    "    return x_train_new, x_test_new, y_train, y_test, preprocessing_dict\n",
    "    \n",
    "\n",
    "\n",
    "x_train_new, x_test_new, y_train, y_test, preprocessing_dict = get_preprocessing_pickles(model_num, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd = [] \n",
    "K = range(1,15)\n",
    "for k in tqdm(K): \n",
    "    km = KMeans(n_clusters = k, max_iter = 1000, tol = 1e-2)\n",
    "    km = km.fit(x_train_new.values)\n",
    "    ssd.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "plt.plot(K, ssd, 'bx-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_steps(preprocessing_dict, x_train, x_test, y_train, y_test, pick_name = None): \n",
    "    #standard scaler volume\n",
    "    x_train_new = pd.DataFrame()\n",
    "    x_test_new = pd.DataFrame()\n",
    "    \n",
    "    x_train_new['Volume'] = preprocessing_dict['ss_volume'].transform(x_train.Volume.values.reshape(-1,1)).ravel()\n",
    "    x_test_new['Volume'] = preprocessing_dict['ss_volume'].transform(x_test.Volume.values.reshape(-1,1)).ravel()\n",
    "    \n",
    "    #standard scaler daydiff\n",
    "    x_train_new['DayDiff'] = preprocessing_dict['ss_daydiff'].transform(x_train.DayDiff.values.reshape(-1,1))\n",
    "    x_test_new['DayDiff'] = preprocessing_dict['ss_daydiff'].transform(x_test.DayDiff.values.reshape(-1,1))\n",
    "    \n",
    "    cv_vec = preprocessing_dict['headlines']\n",
    "    train_headlines = pd.DataFrame(cv_vec.transform(x_train['Headlines']).toarray(), columns = cv_vec.get_feature_names())\n",
    "    test_headlines = pd.DataFrame(cv_vec.transform(x_test['Headlines']).toarray(), columns = cv_vec.get_feature_names())\n",
    "\n",
    "    x_train_new = pd.concat([x_train_new, train_headlines], axis = 1)\n",
    "    x_test_new = pd.concat([x_test_new, test_headlines], axis = 1)\n",
    "\n",
    "    x_test_new['KCluster'] = preprocessing_dict['k_cluster'].transform(x_test_new)\n",
    "    x_train_new['KCluster'] = preprocessing_dict['k_cluster'].transform(x_train_new)\n",
    "\n",
    "    if pick_name: \n",
    "        tts = (x_train_new, x_test_new, y_train, y_test)\n",
    "        pickle.dump(tts, open(f'Pickles/TTS_{pick_name}.p', 'wb'))\n",
    "    return x_train_new, x_test_new, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = preprocess_steps(preprocessing_dict, x_train, x_test, y_train, y_test, pick_name = model_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DemocratRepublicanNLP",
   "language": "python",
   "name": "democratrepublicannlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
