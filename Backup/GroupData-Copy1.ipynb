{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Heeeb\\AppData\\Roaming\\Python\\Python38\\site-packages\\dateutil\\parser\\_parser.py:1213: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "<ipython-input-30-cf4a72aec840>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Combined'] = new_combined\n"
     ]
    }
   ],
   "source": [
    "def combine_headlines_descriptions(df, source): \n",
    "    df = df.dropna(subset = ['Time'])\n",
    "    headlines = np.array([[i] for i in df.Headlines.values])\n",
    "    descriptions = np.array([[i] for i in df.Description.values])\n",
    "    combined = np.concatenate((headlines, descriptions), axis = 1)\n",
    "    new_combined = []\n",
    "    for i in combined: \n",
    "        new_combined.append(' '.join(i))\n",
    "    df['Combined'] = new_combined\n",
    "    df = df.drop(['Headlines', 'Description'], axis = 1)\n",
    "    df['Source'] = [source for i in range(len(df))]\n",
    "    return df\n",
    "\n",
    "def group_csv():\n",
    "    cnbc = pd.read_csv('FData/Headlines/cnbc_headlines.csv')\n",
    "    guardian = pd.read_csv('FData/Headlines/guardian_headlines.csv')\n",
    "    reuters = pd.read_csv('FData/Headlines/reuters_headlines.csv')\n",
    "    stocker_bot = pd.read_csv('FData/Headlines/stockerbot-export1.csv')\n",
    "    reddit = pd.read_csv('FData/Headlines/RedditNews.csv')\n",
    "    \n",
    "    cnbc['Time'] = pd.to_datetime(cnbc['Time'], errors = 'coerce').dt.normalize()\n",
    "    guardian['Time'] = pd.to_datetime(guardian['Time'], errors = 'coerce').dt.normalize()\n",
    "    reuters['Time'] = pd.to_datetime(reuters['Time'], errors = 'coerce').dt.normalize()\n",
    "    \n",
    "    #combining description with the headlines\n",
    "    cnbc = combine_headlines_descriptions(cnbc, source = 'CNBC')\n",
    "    reuters = combine_headlines_descriptions(reuters, source = 'Reuters')\n",
    "    \n",
    "    #renaming guardian and reddit headline for grouping\n",
    "    guardian = guardian.rename(columns = {'Headlines': 'Combined'})\n",
    "    \n",
    "    #combining the datasets \n",
    "    combined_df = pd.concat([guardian, cnbc, reuters, reddit])\n",
    "    \n",
    "    df_dict = {'cnbc': cnbc, 'reuters': reuters, 'reddit': reddit, 'guardian': guardian}\n",
    "    \n",
    "    return df_dict\n",
    "\n",
    "\n",
    "df_dict = group_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Combined</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>Jim Cramer: A better way to invest in the Covi...</td>\n",
       "      <td>CNBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>Cramer's lightning round: I would own Teradyne...</td>\n",
       "      <td>CNBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>Cramer's week ahead: Big week for earnings, ev...</td>\n",
       "      <td>CNBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>IQ Capital CEO Keith Bliss says tech and healt...</td>\n",
       "      <td>CNBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-07-16</td>\n",
       "      <td>Wall Street delivered the 'kind of pullback I'...</td>\n",
       "      <td>CNBC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time                                           Combined Source\n",
       "0 2020-07-17  Jim Cramer: A better way to invest in the Covi...   CNBC\n",
       "1 2020-07-17  Cramer's lightning round: I would own Teradyne...   CNBC\n",
       "3 2020-07-17  Cramer's week ahead: Big week for earnings, ev...   CNBC\n",
       "4 2020-07-17  IQ Capital CEO Keith Bliss says tech and healt...   CNBC\n",
       "5 2020-07-16  Wall Street delivered the 'kind of pullback I'...   CNBC"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict['cnbc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('FData/CombinedHeadlines.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = pd.DataFrame()\n",
    "unique_dates = combined_df.Time.unique()\n",
    "grouped_headlines = []\n",
    "for date in unique_dates: \n",
    "    temp_df = combined_df[combined_df.Time == date]\n",
    "    headlines = temp_df.Combined.values \n",
    "    combined_headlines = ' '.join(headlines)\n",
    "    grouped_headlines.append(combined_headlines)\n",
    "     \n",
    "    \n",
    "grouped_df['Headlines'] = grouped_headlines \n",
    "grouped_df['Time'] = unique_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.sort_values('Time', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy = pd.read_csv('FData/SPYDaily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_seg = spy[(spy.Time >= '2017-12-17') & (spy.Time < '2020-07-18')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_seg = spy_seg.set_index(pd.to_datetime(spy_seg.Time)).drop('Time', axis = 1)\n",
    "grouped_df = grouped_df.set_index(pd.to_datetime(grouped_df.Time)).drop('Time', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_grouped = pd.concat([grouped_df, spy_seg], axis = 1, join = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_grouped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_grouped['CloseDiff'] = spy_grouped['4. close'].diff()\n",
    "spy_grouped = spy_grouped.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_diff = []\n",
    "for diff in spy_grouped.CloseDiff.values[1:]: \n",
    "    close_diff.append(diff)\n",
    "close_diff.append(None)\n",
    "spy_grouped['CloseDiffNew'] = close_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(x): \n",
    "    if x < 0: \n",
    "        return 0 \n",
    "    else: \n",
    "        return 1 \n",
    "\n",
    "spy_grouped['Target'] = spy_grouped.CloseDiffNew.map(get_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_grouped.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_grouped.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_grouped['DayDiff'] = spy_grouped['4. close'] - spy_grouped['1. open'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_grouped = spy_grouped.rename(columns = {'5. volume': 'Volume'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_grouped[['Headlines', 'Volume', 'DayDiff', 'Target']].to_csv('FData/SPYHeadGrouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Vantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from alpha_vantage.timeseries import TimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = config.api_key\n",
    "ticker = 'SPY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries(key = api_key, output_format = 'pandas')\n",
    "\n",
    "data_ts, meta_ts = ts.get_daily(symbol = ticker, outputsize = 'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ts['Time'] = data_ts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ts.to_csv('FData/SPYDaily.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DemocratRepublicanNLP",
   "language": "python",
   "name": "democratrepublicannlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
