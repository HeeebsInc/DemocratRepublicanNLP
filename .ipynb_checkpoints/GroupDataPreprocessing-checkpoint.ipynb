{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Heeeb\\AppData\\Roaming\\Python\\Python38\\site-packages\\dateutil\\parser\\_parser.py:1213: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "<ipython-input-2-c08567d98a4f>:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Combined'] = new_combined\n",
      "Grouping Rows By Dates: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3875/3875 [00:02<00:00, 1316.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_target(x): \n",
    "    if x < 0: \n",
    "        return 0 \n",
    "    else: \n",
    "        return 1 \n",
    "\n",
    "def combine_ticker(ticker_df, grouped_df, path):\n",
    "    ticker_df['Time'] = pd.to_datetime(ticker_df['Time'], errors = 'coerce').dt.normalize()\n",
    "    \n",
    "    #retrieve values that are found in the range of the headlines df\n",
    "    ticker_seg = ticker_df[(ticker_df.Time >= grouped_df.Time.min()) & (ticker_df.Time <= grouped_df.Time.max())]\n",
    "    \n",
    "    #set index equal to each other for merging\n",
    "    ticker_seg = ticker_seg.set_index(pd.to_datetime(ticker_seg.Time)).drop('Time', axis = 1)\n",
    "    grouped_df = grouped_df.set_index(pd.to_datetime(grouped_df.Time)).drop('Time', axis = 1)\n",
    "    \n",
    "    #merge the two df\n",
    "    ticker_grouped = pd.concat([grouped_df, ticker_seg], axis = 1, join = 'inner')\n",
    "    \n",
    "    #get the difference in closing price\n",
    "    ticker_grouped['CloseDiff'] = ticker_grouped['4. close'].diff()\n",
    "    ticker_grouped = ticker_grouped.dropna()\n",
    "    \n",
    "    close_diff = []\n",
    "    for diff in ticker_grouped.CloseDiff.values[1:]: \n",
    "        close_diff.append(diff)\n",
    "    close_diff.append(None)\n",
    "    ticker_grouped['CloseDiffNew'] = close_diff\n",
    "    \n",
    "    #get binary target values for going up or down\n",
    "    ticker_grouped['Target'] = ticker_grouped.CloseDiffNew.map(get_target)\n",
    "    ticker_grouped.dropna(inplace = True)\n",
    "    \n",
    "    #get the difference in price on a day\n",
    "    ticker_grouped['DayDiff'] = ticker_grouped['4. close'] - ticker_grouped['1. open'] \n",
    "    \n",
    "    #rename columns\n",
    "    ticker_grouped = ticker_grouped.rename(columns = {'5. volume': 'Volume'})\n",
    "    ticker_grouped[['Headlines', 'Volume', 'DayDiff', 'Target']]\n",
    "    ticker_grouped.to_csv(f'FData/Headlines/New/{path}')\n",
    "    \n",
    "    return ticker_grouped\n",
    "\n",
    "def group_dates(combined_df): \n",
    "    grouped_df = pd.DataFrame()\n",
    "    unique_dates = combined_df.Time.unique()\n",
    "    grouped_headlines = []\n",
    "    for date in tqdm(unique_dates, desc = 'Grouping Rows By Dates'): \n",
    "        temp_df = combined_df[combined_df.Time == date]\n",
    "        headlines = temp_df.Combined.values \n",
    "        combined_headlines = ' '.join(headlines)\n",
    "        grouped_headlines.append(combined_headlines)\n",
    "     \n",
    "    \n",
    "    grouped_df['Headlines'] = grouped_headlines \n",
    "    grouped_df['Time'] = unique_dates\n",
    "    grouped_df = grouped_df.sort_values('Time', ascending = True).reset_index(drop = True)\n",
    "    grouped_df.dropna(subset = ['Time'], inplace = True)\n",
    "    return grouped_df\n",
    "\n",
    "def get_ticker_df(ticker): \n",
    "    from PyFiles import config\n",
    "    from alpha_vantage.timeseries import TimeSeries\n",
    "    \n",
    "    api_key = config.api_key\n",
    "\n",
    "    ts = TimeSeries(key = api_key, output_format = 'pandas')\n",
    "\n",
    "    data_ts, meta_ts = ts.get_daily(symbol = ticker, outputsize = 'full')\n",
    "\n",
    "    data_ts['Time'] = data_ts.index\n",
    "    data_ts.to_csv(f'FData/Headlines/New/{ticker}Daily.csv', index = False)\n",
    "    \n",
    "    return data_ts\n",
    "\n",
    "def remove_b(x): \n",
    "    if x[:2] == 'b\"' or x[:2] == \"b'\": \n",
    "        x = x[1::]\n",
    "        return x\n",
    "    else: \n",
    "        return x\n",
    "def combine_headlines_descriptions(df): \n",
    "    df = df.dropna(subset = ['Time'])\n",
    "    headlines = np.array([[i] for i in df.Headlines.values])\n",
    "    descriptions = np.array([[i] for i in df.Description.values])\n",
    "    combined = np.concatenate((headlines, descriptions), axis = 1)\n",
    "    new_combined = []\n",
    "    for i in combined: \n",
    "        new_combined.append(' '.join(i))\n",
    "    df['Combined'] = new_combined\n",
    "    df = df.drop(['Headlines', 'Description'], axis = 1)\n",
    "    return df\n",
    "\n",
    "def combine_headlines(path, ticker):\n",
    "    cnbc = pd.read_csv('FData/Headlines/cnbc_headlines.csv')\n",
    "    guardian = pd.read_csv('FData/Headlines/guardian_headlines.csv')\n",
    "    reuters = pd.read_csv('FData/Headlines/reuters_headlines.csv')\n",
    "    stocker_bot = pd.read_csv('FData/Headlines/stockerbot-export1.csv')\n",
    "    reddit = pd.read_csv('FData/Headlines/RedditNews.csv')\n",
    "    \n",
    "    cnbc['Time'] = pd.to_datetime(cnbc['Time'], errors = 'coerce').dt.normalize()\n",
    "    guardian['Time'] = pd.to_datetime(guardian['Time'], errors = 'coerce').dt.normalize()\n",
    "    reuters['Time'] = pd.to_datetime(reuters['Time'], errors = 'coerce').dt.normalize()\n",
    "    \n",
    "    #combining description with the headlines\n",
    "    cnbc = combine_headlines_descriptions(cnbc)\n",
    "    reuters = combine_headlines_descriptions(reuters)\n",
    "    \n",
    "    #renaming guardian and reddit headline for grouping\n",
    "    guardian = guardian.rename(columns = {'Headlines': 'Combined'})\n",
    "    reddit = reddit.rename(columns = {'Headlines': 'Combined'})\n",
    "    \n",
    "    #adding source to each df\n",
    "    guardian['Source'] = ['Guardian' for i in range(len(guardian))]\n",
    "    cnbc['Source'] = ['CNBC' for i in range(len(cnbc))]\n",
    "    reddit['Source'] = ['Reddit' for i in range(len(reddit))]\n",
    "    reuters['Source'] = ['Reuters' for i in range(len(reuters))]\n",
    "    \n",
    "    #decoding the strings in reddit \n",
    "    reddit['Combined'] = reddit.Combined.map(remove_b)\n",
    "    \n",
    "    #combining the datasets \n",
    "    combined_df = pd.concat([guardian, cnbc, reuters, reddit])\n",
    "    \n",
    "    df_dict = {'cnbc': cnbc, 'reuters': reuters, 'reddit': reddit, 'guardian': guardian, 'combined': combined_df}\n",
    "    \n",
    "    combined_df['Time'] = pd.to_datetime(combined_df['Time'], errors = 'coerce').dt.normalize()\n",
    "    \n",
    "    #save combinedHeadlines\n",
    "    combined_df.to_csv(f'FData/Headlines/New/CombinedHeadlines.csv')\n",
    "    \n",
    "    #group the headlines by day\n",
    "    grouped_df = group_dates(combined_df)\n",
    "    \n",
    "    #get the ticker df\n",
    "    ticker_df = get_ticker_df(ticker) \n",
    "    \n",
    "    #join the ticker_df and headline df by day\n",
    "    ticker_grouped = combine_ticker(ticker_df, grouped_df, path)\n",
    "\n",
    "    return ticker_grouped\n",
    "\n",
    "ticker = 'SPY'\n",
    "ticker_grouped = combine_headlines(f'{ticker}HeadlinesGrouped.csv', ticker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_headline(headline, pre_type): \n",
    "    reg_token = RegexpTokenizer(\"([a-zA-Z&]+(?:'[a-z]+)?)\")\n",
    "\n",
    "    new_headline = ' '.join([i for i in headline.lower().split() if i != 'rt' and i.endswith('…') == False])\n",
    "    new_headline  = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",new_headline.lower()).split())\n",
    "    new_headline  = reg_token.tokenize(new_headline .lower())\n",
    "    \n",
    "    if pre_type == 'stem': \n",
    "        word_stem = PorterStemmer()\n",
    "        new_headline = [word_stem.stem(i) for i in new_headline if len(i) > 1]\n",
    "    elif pre_type == 'lemmet': \n",
    "        word_lem = WordNetLemmatizer()\n",
    "        new_headline= [word_lem.lemmatize(i) for i in new_headline if len(i) > 1]\n",
    "    \n",
    "    return ' '.join(new_headline)\n",
    "\n",
    "\n",
    "def preprocess_steps(preprocessing_dict, x_train, x_test, y_train, y_test, kmeans_cluster = None): \n",
    "    #standard scaler volume\n",
    "    x_train_new = pd.DataFrame()\n",
    "    x_test_new = pd.DataFrame()\n",
    "    \n",
    "    x_train_new['Volume'] = preprocessing_dict['ss_volume'].transform(x_train.Volume.values.reshape(-1,1)).ravel()\n",
    "    x_test_new['Volume'] = preprocessing_dict['ss_volume'].transform(x_test.Volume.values.reshape(-1,1)).ravel()\n",
    "    \n",
    "    #standard scaler daydiff\n",
    "    x_train_new['DayDiff'] = preprocessing_dict['ss_daydiff'].transform(x_train.DayDiff.values.reshape(-1,1))\n",
    "    x_test_new['DayDiff'] = preprocessing_dict['ss_daydiff'].transform(x_test.DayDiff.values.reshape(-1,1))\n",
    "    \n",
    "    cv_vec = preprocessing_dict['headlines']\n",
    "    train_headlines = pd.DataFrame(cv_vec.transform(x_train['Headlines']).toarray(), columns = cv_vec.get_feature_names())\n",
    "    test_headlines = pd.DataFrame(cv_vec.transform(x_test['Headlines']).toarray(), columns = cv_vec.get_feature_names())\n",
    "\n",
    "    x_train_new = pd.concat([x_train_new, train_headlines], axis = 1)\n",
    "    x_test_new = pd.concat([x_test_new, test_headlines], axis = 1)\n",
    "    \n",
    "    if kmeans_cluster: \n",
    "        kmeans = KMeans(n_clusters = kmeans_cluster, max_iter = 1000, tol = 1e-3).fit(x_train_new.values)\n",
    "        preprocessing_dict['k_cluster'] = kmeans\n",
    "        x_test_new['KCluster'] = kmeans.predict (x_test_new.values)\n",
    "        x_train_new['KCluster'] = kmeans.predict(x_train_new.values)\n",
    "\n",
    "    return x_train_new, x_test_new, y_train, y_test, preprocessing_dict\n",
    "\n",
    "\n",
    "def get_preprocessing_pickles(x_train, x_test, y_train, y_test, ngram, max_features, min_df, max_df):\n",
    "    ss_volume = StandardScaler().fit(x_train['Volume'].values.reshape(-1,1))\n",
    "    ss_daydiff = StandardScaler().fit(x_train['DayDiff'].values.reshape(-1,1))\n",
    "    cv = CountVectorizer(stop_words = 'english', max_features = max_features, ngram_range = ngram, min_df = min_df,\n",
    "                                     max_df = max_df).fit(x_train['Headlines'])\n",
    "\n",
    "    preprocessing_dict = {'ss_volume': ss_volume, 'ss_daydiff': ss_daydiff, 'headlines': cv}\n",
    "    \n",
    "    x_train_new, x_test_new, y_train, y_test, preprocessing_dict = preprocess_steps(preprocessing_dict, x_train, x_test, \n",
    "                                                                                    y_train, y_test)\n",
    "    \n",
    "    return x_train_new, x_test_new, y_train, y_test, preprocessing_dict\n",
    "    \n",
    "def preprocess_tts(df, pre_type, ngram, max_features, min_df, max_df): \n",
    "    df['Headlines'] = df.Headlines.apply(preprocess_headline, pre_type = pre_type)\n",
    "\n",
    "    X = df[['Headlines', 'Volume', 'DayDiff']]\n",
    "    Y = df[['Target']]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,Y, stratify = Y.Target.values, random_state = 10, \n",
    "                                                        train_size = .70)\n",
    "    print(f'Train:\\t{len(x_train)}\\n{y_train.Target.value_counts()}\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(f'Test:\\t{len(x_test)}\\n{y_test.Target.value_counts()}\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    \n",
    "    #preprocess the split and return the objects\n",
    "    x_train_new, x_test_new, y_train, y_test, preprocessing_dict = get_preprocessing_pickles(x_train, \n",
    "                                                                                             x_test, y_train, y_test, ngram, \n",
    "                                                                                            max_features, min_df, max_df)\n",
    "    \n",
    "    return df, x_train_new, x_test_new, y_train, y_test, preprocessing_dict\n",
    "\n",
    "    \n",
    "df = pd.read_csv('FData/Headlines/New/SPYHeadlinesGrouped.csv')\n",
    "new_df, x_train, x_test, y_train, y_test, preprocessing_dict = preprocess_tts(df, pre_type = 'stem', ngram = (1,1), max_features = 5000, \n",
    "                                                                             min_df = 1, max_df =1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DemocratRepublicanNLP",
   "language": "python",
   "name": "democratrepublicannlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
