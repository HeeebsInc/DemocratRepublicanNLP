{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(x): \n",
    "    if x < 0: \n",
    "        return 0 \n",
    "    elif x >= 0: \n",
    "        return 1 \n",
    "    else: \n",
    "        return None\n",
    "\n",
    "# def get_target(x): \n",
    "#     if abs(x) >= 0 and abs(x) < 1: \n",
    "#         return 1 \n",
    "#     elif x >= 1: \n",
    "#         return 2 \n",
    "#     elif x <= -1: \n",
    "#         return 0\n",
    "    \n",
    "#     else: \n",
    "#         return None\n",
    "\n",
    "def combine_ticker(ticker_df, grouped_df, path):\n",
    "    ticker_df['Time'] = pd.to_datetime(ticker_df['Time'], errors = 'coerce').dt.normalize()\n",
    "    \n",
    "    #retrieve values that are found in the range of the headlines df\n",
    "    ticker_seg = ticker_df[(ticker_df.Time >= grouped_df.Time.min()) & (ticker_df.Time <= grouped_df.Time.max())]\n",
    "    \n",
    "    #set index equal to each other for merging\n",
    "    ticker_seg = ticker_seg.set_index(pd.to_datetime(ticker_seg.Time)).drop('Time', axis = 1)\n",
    "    grouped_df = grouped_df.set_index(pd.to_datetime(grouped_df.Time)).drop('Time', axis = 1)\n",
    "    \n",
    "    #merge the two df\n",
    "    ticker_grouped = pd.concat([grouped_df, ticker_seg], axis = 1, join = 'inner')\n",
    "    \n",
    "    #get the difference in closing price\n",
    "    ticker_grouped['CloseDiff'] = ticker_grouped['4. close'].diff()\n",
    "    \n",
    "    close_diff = []\n",
    "    for diff in ticker_grouped.CloseDiff.values[1:]: \n",
    "        close_diff.append(diff)\n",
    "    close_diff.append(None)\n",
    "    ticker_grouped['CloseDiffNew'] = close_diff\n",
    "    \n",
    "    #get binary target values for going up or down\n",
    "    ticker_grouped['Target'] = ticker_grouped.CloseDiffNew.map(get_target)\n",
    "    ticker_grouped.dropna(subset = ['Target', 'CloseDiffNew'],inplace = True)\n",
    "    \n",
    "    #get the difference in price on a day\n",
    "    ticker_grouped['DayDiff'] = ticker_grouped['4. close'] - ticker_grouped['1. open'] \n",
    "    \n",
    "    #rename columns\n",
    "    ticker_grouped = ticker_grouped.rename(columns = {'5. volume': 'Volume'})\n",
    "    ticker_grouped[['Headlines', 'Volume', 'DayDiff', 'Target']]\n",
    "    ticker_grouped.to_csv(f'FData/Headlines/New/{path}')\n",
    "    \n",
    "    return ticker_grouped\n",
    "\n",
    "def group_dates(combined_df): \n",
    "    grouped_df = pd.DataFrame()\n",
    "    unique_dates = combined_df.Time.unique()\n",
    "    grouped_headlines = []\n",
    "    pbar = tqdm(unique_dates, desc = 'Grouping Rows By Dates')\n",
    "    for date in pbar: \n",
    "        temp_df = combined_df[combined_df.Time == date]\n",
    "        headlines = temp_df.Combined.values \n",
    "        combined_headlines = ' '.join(headlines)\n",
    "        grouped_headlines.append(combined_headlines)\n",
    "     \n",
    "    pbar.close()\n",
    "    grouped_df['Headlines'] = grouped_headlines \n",
    "    grouped_df['Time'] = unique_dates\n",
    "    grouped_df = grouped_df.sort_values('Time', ascending = True).reset_index(drop = True)\n",
    "    grouped_df.dropna(subset = ['Time'], inplace = True)\n",
    "    return grouped_df\n",
    "\n",
    "def get_ticker_df(ticker): \n",
    "    from PyFiles import config\n",
    "    from alpha_vantage.timeseries import TimeSeries\n",
    "    \n",
    "    api_key = config.api_key\n",
    "\n",
    "    ts = TimeSeries(key = api_key, output_format = 'pandas')\n",
    "\n",
    "    data_ts, meta_ts = ts.get_daily(symbol = ticker, outputsize = 'full')\n",
    "\n",
    "    data_ts['Time'] = data_ts.index\n",
    "    data_ts.to_csv(f'FData/Headlines/New/{ticker}Daily.csv', index = False)\n",
    "    \n",
    "    return data_ts\n",
    "\n",
    "def remove_b(x): \n",
    "    if x[:2] == 'b\"' or x[:2] == \"b'\": \n",
    "        x = x[1::]\n",
    "        return x\n",
    "    else: \n",
    "        return x\n",
    "def combine_headlines_descriptions(df): \n",
    "    df = df.dropna(subset = ['Time'])\n",
    "    headlines = np.array([[i] for i in df.Headlines.values])\n",
    "    descriptions = np.array([[i] for i in df.Description.values])\n",
    "    combined = np.concatenate((headlines, descriptions), axis = 1)\n",
    "    new_combined = []\n",
    "    for i in combined: \n",
    "        new_combined.append(' '.join(i))\n",
    "    df['Combined'] = new_combined\n",
    "    df = df.drop(['Headlines', 'Description'], axis = 1)\n",
    "    return df\n",
    "\n",
    "def combine_headlines(ticker, path = None):\n",
    "    cnbc = pd.read_csv('FData/Headlines/cnbc_headlines.csv')\n",
    "    guardian = pd.read_csv('FData/Headlines/guardian_headlines.csv')\n",
    "    reuters = pd.read_csv('FData/Headlines/reuters_headlines.csv')\n",
    "    reddit = pd.read_csv('FData/Headlines/RedditNews.csv')\n",
    "#     stocker_bot = pd.read_csv('FData/Headlines/stockerbot-export1.csv')\n",
    "\n",
    "    \n",
    "    cnbc['Time'] = pd.to_datetime(cnbc['Time'], errors = 'coerce').dt.normalize()\n",
    "    guardian['Time'] = pd.to_datetime(guardian['Time'], errors = 'coerce').dt.normalize()\n",
    "    reuters['Time'] = pd.to_datetime(reuters['Time'], errors = 'coerce').dt.normalize()\n",
    "    \n",
    "    #combining description with the headlines\n",
    "    cnbc = combine_headlines_descriptions(cnbc)\n",
    "    reuters = combine_headlines_descriptions(reuters)\n",
    "    \n",
    "    #renaming guardian and reddit headline for grouping\n",
    "    guardian = guardian.rename(columns = {'Headlines': 'Combined'})\n",
    "    reddit = reddit.rename(columns = {'Headlines': 'Combined'})\n",
    "    \n",
    "    #adding source to each df\n",
    "    guardian['Source'] = ['Guardian' for i in range(len(guardian))]\n",
    "    cnbc['Source'] = ['CNBC' for i in range(len(cnbc))]\n",
    "    reddit['Source'] = ['Reddit' for i in range(len(reddit))]\n",
    "    reuters['Source'] = ['Reuters' for i in range(len(reuters))]\n",
    "    \n",
    "    #decoding the strings in reddit \n",
    "    reddit['Combined'] = reddit.Combined.map(remove_b)\n",
    "    \n",
    "    #combining the datasets \n",
    "    combined_df = pd.concat([guardian, cnbc, reuters, reddit])\n",
    "    \n",
    "    df_dict = {'cnbc': cnbc, 'reuters': reuters, 'reddit': reddit, 'guardian': guardian, 'combined': combined_df}\n",
    "    \n",
    "    combined_df['Time'] = pd.to_datetime(combined_df['Time'], errors = 'coerce').dt.normalize()\n",
    "    \n",
    "    #save combinedHeadlines\n",
    "    if path:\n",
    "        combined_df.to_csv(f'FData/Headlines/New/{path}')\n",
    "    \n",
    "    #group the headlines by day\n",
    "    grouped_df = group_dates(combined_df)\n",
    "    \n",
    "    #get the ticker df\n",
    "    ticker_df = get_ticker_df(ticker) \n",
    "    \n",
    "    #join the ticker_df and headline df by day\n",
    "    ticker_grouped = combine_ticker(ticker_df, grouped_df, path)\n",
    "    ticker_grouped = ticker_grouped.reset_index()\n",
    "    return ticker_grouped, df_dict\n",
    "\n",
    "ticker = 'SPY'\n",
    "ticker_grouped, df_dict = combine_headlines(ticker, path = f'{ticker}HeadlinesGrouped1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_headline(headline, pre_type = None): \n",
    "    reg_token = RegexpTokenizer(\"([a-zA-Z&]+(?:'[a-z]+)?)\")\n",
    "\n",
    "    new_headline = ' '.join([i for i in headline.lower().split() if i != 'rt' and i.endswith('â€¦') == False])\n",
    "    new_headline  = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",new_headline.lower()).split())\n",
    "    new_headline  = reg_token.tokenize(new_headline .lower())\n",
    "    \n",
    "    if pre_type == 'stem': \n",
    "        word_stem = PorterStemmer()\n",
    "        new_headline = [word_stem.stem(i) for i in new_headline if len(i) > 1]\n",
    "    elif pre_type == 'lemmet': \n",
    "        word_lem = WordNetLemmatizer()\n",
    "        new_headline= [word_lem.lemmatize(i) for i in new_headline if len(i) > 1]\n",
    "    \n",
    "    return ' '.join(new_headline)\n",
    "\n",
    "\n",
    "def preprocess_steps(preprocessing_dict, x_train, x_test, y_train, y_test, kmeans_cluster = None): \n",
    "    #standard scaler volume\n",
    "    x_train_new = pd.DataFrame()\n",
    "    x_test_new = pd.DataFrame()\n",
    "    \n",
    "    x_train_new['Volume'] = preprocessing_dict['ss_volume'].transform(x_train.Volume.values.reshape(-1,1)).ravel()\n",
    "    x_test_new['Volume'] = preprocessing_dict['ss_volume'].transform(x_test.Volume.values.reshape(-1,1)).ravel()\n",
    "    \n",
    "    #standard scaler daydiff\n",
    "    x_train_new['DayDiff'] = preprocessing_dict['ss_daydiff'].transform(x_train.DayDiff.values.reshape(-1,1)).ravel()\n",
    "    x_test_new['DayDiff'] = preprocessing_dict['ss_daydiff'].transform(x_test.DayDiff.values.reshape(-1,1)).ravel()\n",
    "    \n",
    "    #standardscaler low\n",
    "    x_train_new['Low'] = preprocessing_dict['ss_low'].transform(x_train['3. low'].values.reshape(-1,1)).ravel()\n",
    "    x_test_new['Low'] = preprocessing_dict['ss_low'].transform(x_test['3. low'].values.reshape(-1,1)).ravel()\n",
    "    \n",
    "    vect = preprocessing_dict['headlines']\n",
    "    train_headlines = pd.DataFrame(vect.transform(x_train['Headlines']).toarray(), columns = vect.get_feature_names())\n",
    "    test_headlines = pd.DataFrame(vect.transform(x_test['Headlines']).toarray(), columns = vect.get_feature_names())\n",
    "\n",
    "    x_train_new = pd.concat([x_train_new, train_headlines], axis = 1)\n",
    "    x_test_new = pd.concat([x_test_new, test_headlines], axis = 1)\n",
    "    \n",
    "    if kmeans_cluster: \n",
    "        kmeans = KMeans(n_clusters = kmeans_cluster, max_iter = 1000, tol = 1e-3).fit(x_train_new.values)\n",
    "        preprocessing_dict['k_cluster'] = kmeans\n",
    "        x_test_new['KCluster'] = kmeans.predict (x_test_new.values)\n",
    "        x_train_new['KCluster'] = kmeans.predict(x_train_new.values)\n",
    "\n",
    "    return x_train_new, x_test_new, y_train, y_test\n",
    "\n",
    "\n",
    "def get_preprocessing_objects(x_train, ngram, max_features, min_df, max_df, vect_type = 'cv'):\n",
    "    ss_volume = StandardScaler().fit(x_train['Volume'].values.reshape(-1,1))\n",
    "    ss_daydiff = StandardScaler().fit(x_train['DayDiff'].values.reshape(-1,1))\n",
    "    ss_low = StandardScaler().fit(x_train['3. low'].values.reshape(-1,1))\n",
    "    sw = stopwords.words('english')\n",
    "    if vect_type == 'cv': \n",
    "        vect = CountVectorizer(stop_words = sw, max_features = max_features, ngram_range = ngram, min_df = min_df,\n",
    "                                     max_df = max_df).fit(x_train['Headlines'])\n",
    "    elif vect_type == 'tfidf': \n",
    "        vect = TfidfVectorizer(stop_words = sw, max_features = max_features, ngram_range = ngram, min_df = min_df,\n",
    "                                     max_df = max_df).fit(x_train['Headlines'])\n",
    "\n",
    "    preprocessing_dict = {'ss_volume': ss_volume, 'ss_daydiff': ss_daydiff, 'ss_low': ss_low, 'headlines': vect}\n",
    "     \n",
    "    return preprocessing_dict\n",
    "    \n",
    "def preprocess_tts(df, pre_type, ngram, max_features, min_df, max_df, vect_type): \n",
    "    new_df = df.copy()\n",
    "    pbar = tqdm(total = 3)\n",
    "    \n",
    "    pbar.set_description('Preprocessing Headlines')\n",
    "    new_df['Headlines'] = new_df.Headlines.apply(preprocess_headline, pre_type = pre_type)\n",
    "#     return new_df, 1,1,1,1,1\n",
    "    pbar.update(1)\n",
    "    \n",
    "    X = new_df[['Headlines', 'Volume', 'DayDiff', '3. low']]\n",
    "    Y = new_df[['Target']]\n",
    "    \n",
    "    pbar.set_description('Applying Train Test Split')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,Y, stratify = Y.Target.values, random_state = 10, \n",
    "                                                        train_size = .8)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    \n",
    "    #preprocess the split and return the objects\n",
    "    pbar.set_description('Getting Preprocessing Objects and Transforming Data')\n",
    "    #getting the transformers\n",
    "    preprocessing_dict = get_preprocessing_objects(x_train, ngram, max_features, min_df, max_df, vect_type)  \n",
    "    #applying transformers to data \n",
    "    x_train_new, x_test_new, y_train, y_test = preprocess_steps(preprocessing_dict, x_train, x_test, \n",
    "                                                                                    y_train, y_test)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    print(f'Train:\\t{len(x_train)}\\n{y_train.Target.value_counts()}\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(f'Test:\\t{len(x_test)}\\n{y_test.Target.value_counts()}\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    y_train = y_train.values.ravel() \n",
    "    y_test = y_test.values.ravel()\n",
    "    pbar.close()\n",
    "    return new_df, x_train_new, x_test_new, y_train, y_test, preprocessing_dict\n",
    "\n",
    "    \n",
    "# df = pd.read_csv('FData/Headlines/New/SPYHeadlinesGrouped.csv')\n",
    "# df = pd.read_csv('FData/SPYHeadGrouped.csv')\n",
    "df = ticker_grouped \n",
    "new_df, x_train, x_test, y_train, y_test, preprocessing_dict = preprocess_tts(df, pre_type = 'lemmet', ngram = (1,2), \n",
    "                                                                              max_features = None, \n",
    "                                                                             min_df = 10, max_df =1.0, vect_type = 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in new_df.CloseDiffNew.values: \n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier, RidgeClassifier, LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from PyFiles import Functions as func\n",
    "from PyFiles import Preprocessing as process\n",
    "\n",
    "\n",
    "models = {'Log': LogisticRegression(max_iter = 2500), 'Knn': KNeighborsClassifier(), \n",
    "          'DT': DecisionTreeClassifier(random_state = 10), 'LDA': LinearDiscriminantAnalysis(),\n",
    "          'Gaussian': GaussianNB(), \n",
    "           'LinearSVC': LinearSVC(max_iter = 2500, random_state = 10), 'SDGSVC': SGDClassifier(random_state = 10),  \n",
    "          'ADA': AdaBoostClassifier(random_state = 10), 'Bagging': BaggingClassifier(random_state = 10), \n",
    "          'Ridge': RidgeClassifier(random_state = 10), 'RF': RandomForestClassifier(random_state = 10)}\n",
    "\n",
    "new_models = func.stacked_model(models)\n",
    "stacked = new_models['Stacked']\n",
    "stacked.n_jobs = -1\n",
    "stacked.fit(x_train, y_train)\n",
    "print(f'Train Accuracy: {stacked.score(x_train, y_train)}')\n",
    "print(f'Test Accuracy: {stacked.score(x_test, y_test)}')\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "# plt.suptitle(f'Model: {model_num}')\n",
    "plot_confusion_matrix(stacked, x_train, y_train, ax = ax[0], display_labels = ['Down', 'Up']) \n",
    "plot_confusion_matrix(stacked, x_test, y_test, ax = ax[1], display_labels = ['Down', 'Up']) \n",
    "ax[0].set_title('Train')\n",
    "ax[1].set_title('Test')\n",
    "plt.tight_layout() \n",
    "# plt.savefig(f'Images/Sklearn/CM_{model_num}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DemocratRepublicanNLP",
   "language": "python",
   "name": "democratrepublicannlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
